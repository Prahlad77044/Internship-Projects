{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0794184",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00599a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88edd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df7a683",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d5c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = pd.read_csv(r\"C:\\Users\\Acer\\Desktop\\projects\\week_4\\train.csv\")\n",
    "test_df = pd.read_csv(r\"C:\\Users\\Acer\\Desktop\\projects\\week_4\\test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1eb36d",
   "metadata": {},
   "source": [
    "# Train/test/val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d546675",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=0.1,\n",
    "    random_state=SEED,\n",
    "    stratify=train_df[\"label\"]\n",
    ")\n",
    "print(f\"Train size: {len(train_df)}, Val size: {len(val_df)}, Test size: {len(test_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac00cb4d",
   "metadata": {},
   "source": [
    "# Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a74f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"<.*?>\", \" \", text)           # remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z']\", \" \", text)      # keep letters only\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()     # remove extra spaces\n",
    "    return text\n",
    "\n",
    "train_df[\"text\"] = train_df[\"text\"].apply(clean_text)\n",
    "val_df[\"text\"] = val_df[\"text\"].apply(clean_text)\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec4cae1",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc0d7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
    "X_train = vectorizer.fit_transform(train_df[\"text\"])\n",
    "X_val = vectorizer.transform(val_df[\"text\"])\n",
    "X_test = vectorizer.transform(test_df[\"text\"])\n",
    "\n",
    "y_train = train_df[\"label\"]\n",
    "y_val = val_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423d0fef",
   "metadata": {},
   "source": [
    "# Model definition , training and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8e90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=500, random_state=SEED)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predict on test set\n",
    "log_test_pred = clf.predict(X_test)\n",
    "\n",
    "# Macro F1 score\n",
    "print(\"Macro F1:\", f1_score(y_test, log_test_pred, average=\"macro\"))\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, log_test_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, log_test_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "\n",
    "# Optional: Visualize confusion matrix\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0455da",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16315b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(clf, \"logreg_model.joblib\")\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.joblib\")\n",
    "print(\"Model and vectorizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb626b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------------------\n",
    "# 1. Train SVM model\n",
    "# -----------------------------------------\n",
    "svm_model = LinearSVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------------------\n",
    "# 3. Final test evaluation\n",
    "# -----------------------------------------\n",
    "svm_test_preds = svm_model.predict(X_test)\n",
    "\n",
    "# Accuracy and Macro-F1\n",
    "test_acc = accuracy_score(y_test, svm_test_preds)\n",
    "test_f1 = f1_score(y_test, svm_test_preds, average=\"macro\")\n",
    "\n",
    "print(\"\\nTest Accuracy:\", test_acc)\n",
    "print(\"Test Macro-F1:\", test_f1)\n",
    "\n",
    "print(\"\\nCLASSIFICATION REPORT (TEST):\")\n",
    "print(classification_report(y_test, svm_test_preds))\n",
    "\n",
    "# -----------------------------------------\n",
    "# 4. Confusion Matrix\n",
    "# -----------------------------------------\n",
    "cm = confusion_matrix(y_test, svm_test_preds)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "\n",
    "# Optional: visualize confusion matrix\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=np.unique(y_test),\n",
    "            yticklabels=np.unique(y_test))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf1bee8",
   "metadata": {},
   "source": [
    "# Error Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28586fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_error_buckets(df, preds, model_name):\n",
    "    text = df[\"text\"].values\n",
    "    true = df[\"label\"].values\n",
    "\n",
    "    buckets = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        t = true[i]\n",
    "        p = preds[i]\n",
    "\n",
    "        if t == 1 and p == 1:\n",
    "            bucket = \"TP (Correct Positive)\"\n",
    "        elif t == 0 and p == 0:\n",
    "            bucket = \"TN (Correct Negative)\"\n",
    "        elif t == 0 and p == 1:\n",
    "            bucket = \"FP (Predicted Positive but Actually Negative)\"\n",
    "        elif t == 1 and p == 0:\n",
    "            bucket = \"FN (Predicted Negative but Actually Positive)\"\n",
    "\n",
    "        buckets.append((model_name, text[i], t, p, bucket))\n",
    "\n",
    "    return pd.DataFrame(buckets, columns=[\"Model\", \"Text\", \"True\", \"Pred\", \"Bucket\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca966213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For SVM\n",
    "svm_test_errors = create_error_buckets(test_df, svm_test_preds, \"SVM\")\n",
    "\n",
    "# For Logistic Regression\n",
    "log_test_errors = create_error_buckets(test_df, log_test_pred, \"Logistic Regression\")\n",
    "\n",
    "# Combine both\n",
    "all_test_errors = pd.concat([svm_test_errors, log_test_errors], axis=0)\n",
    "\n",
    "all_test_errors.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdc9607",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_errors.groupby([\"Model\", \"Bucket\"]).size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd284259",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_errors[(all_test_errors[\"Bucket\"].str.startswith(\"FP\"))].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test_errors[(all_test_errors[\"Bucket\"].str.startswith(\"FN\"))].head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
