{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e9e6ab2",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bfa1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torchvision import datasets, transforms,models\n",
    "from torchvision.transforms import RandAugment\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.optim import Adam\n",
    "import torch.optim as optim\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import copy\n",
    "import time\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "\n",
    "\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77015ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87538eca",
   "metadata": {},
   "source": [
    "# Load and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c801ebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "CIFAR10_MEAN = [0.4914, 0.4822, 0.4465]\n",
    "CIFAR10_STD  = [0.2470, 0.2435, 0.2616]\n",
    "\n",
    "def get_transforms(use_randaugment=False, use_cutout=False):\n",
    "    # --- Training transforms (with augmentation) ---\n",
    "    train_tf_list = [\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),           # crop to 224×224\n",
    "        transforms.RandomHorizontalFlip()\n",
    "    ]\n",
    "    \n",
    "    if use_randaugment:\n",
    "        from torchvision.transforms import RandAugment\n",
    "        train_tf_list.append(RandAugment())\n",
    "    \n",
    "    train_tf_list.append(transforms.ToTensor())\n",
    "    \n",
    "    if use_cutout:\n",
    "        class Cutout:\n",
    "            def __init__(self, n_holes=1, length=16):\n",
    "                self.n_holes = n_holes\n",
    "                self.length = length\n",
    "            def __call__(self, img):\n",
    "                h, w = img.shape[1], img.shape[2]\n",
    "                mask = torch.ones((h, w), dtype=torch.float32)\n",
    "                import numpy as np\n",
    "                for _ in range(self.n_holes):\n",
    "                    y = np.random.randint(h)\n",
    "                    x = np.random.randint(w)\n",
    "                    y1 = np.clip(y - self.length // 2, 0, h)\n",
    "                    y2 = np.clip(y + self.length // 2, 0, h)\n",
    "                    x1 = np.clip(x - self.length // 2, 0, w)\n",
    "                    x2 = np.clip(x + self.length // 2, 0, w)\n",
    "                    mask[y1:y2, x1:x2] = 0.0\n",
    "                mask = mask.expand_as(img)\n",
    "                return img * mask\n",
    "        train_tf_list.append(Cutout(n_holes=1, length=16))\n",
    "    \n",
    "    train_tf_list.append(transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD))\n",
    "    train_transform = transforms.Compose(train_tf_list)\n",
    "    \n",
    "    # --- Validation/Test transforms (no augmentation) ---\n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_test_transform\n",
    "\n",
    "def get_dataloaders(batch_size=128, use_randaugment=False, use_cutout=False, num_workers=4, val_split=0.1):\n",
    "    train_tf, val_test_tf = get_transforms(use_randaugment, use_cutout)\n",
    "    \n",
    "    # Load the full CIFAR-10 train set with training transforms (including augmentations)\n",
    "    full_train = datasets.CIFAR10(\"./data\", train=True, download=True, transform=train_tf)\n",
    "    n = len(full_train)\n",
    "    val_size = int(n * val_split)\n",
    "    train_size = n - val_size\n",
    "    \n",
    "    # Split indices\n",
    "    train_indices, val_indices = random_split(list(range(n)), [train_size, val_size])\n",
    "    \n",
    "    # Create subsets\n",
    "    train_set = Subset(full_train, train_indices)\n",
    "    \n",
    "    # For validation set, use clean transform (no augmentation)\n",
    "    # So create a new dataset instance with val/test transform\n",
    "    full_val = datasets.CIFAR10(\"./data\", train=True, download=False, transform=val_test_tf)\n",
    "    val_set = Subset(full_val, val_indices)\n",
    "    \n",
    "    # Test set\n",
    "    test_set = datasets.CIFAR10(\"./data\", train=False, download=True, transform=val_test_tf)\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader   = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    test_loader  = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181d3c3e",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9908af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = models.resnet18(pretrained=True)\n",
    "num_classes = 10  # CIFAR-10\n",
    "model_base.fc = nn.Linear(model_base.fc.in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e3c340",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad12568f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_mlflow(model, train_loader, val_loader, epochs=5, lr=1e-3, device='cuda',\n",
    "                       experiment_name=\"resnet_experiments\", optimizer_type=\"SGD\"):\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    with mlflow.start_run(run_name=f\"training_run_{int(time.time())}\"):\n",
    "\n",
    "        mlflow.log_params({\"epochs\": epochs, \"lr\": lr, \"optimizer\": optimizer_type})\n",
    "        model.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "        if optimizer_type.lower() == \"sgd\":\n",
    "            optimizer = optim.SGD(trainable_params, lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "        elif optimizer_type.lower() == \"adam\":\n",
    "            optimizer = optim.Adam(trainable_params, lr=lr, weight_decay=5e-4)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer type\")\n",
    "\n",
    "        history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "        start_time = time.time()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            running_loss, correct, total = 0.0, 0, 0\n",
    "            for inputs, labels in train_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            train_loss = running_loss / total\n",
    "            train_acc = correct / total\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item() * inputs.size(0)\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    val_total += labels.size(0)\n",
    "                    val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            val_loss /= val_total\n",
    "            val_acc = val_correct / val_total\n",
    "\n",
    "            # Save metrics\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['train_acc'].append(train_acc)\n",
    "            history['val_acc'].append(val_acc)\n",
    "\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}] Train Loss: {train_loss:.4f}, \"\n",
    "                  f\"Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "            mlflow.log_metrics({\"train_loss\": train_loss, \"train_acc\": train_acc,\n",
    "                                \"val_loss\": val_loss, \"val_acc\": val_acc}, step=epoch)\n",
    "\n",
    "        total_time = time.time() - start_time\n",
    "        mlflow.log_metric(\"training_time_sec\", total_time)\n",
    "        mlflow.pytorch.log_model(model, \"final_model\")\n",
    "        print(f\"Model saved to MLflow. Training time: {total_time:.2f} sec\")\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9497fc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure this is executed before training\n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    batch_size=128,\n",
    "    use_randaugment=True,\n",
    "    use_cutout=False,\n",
    "    num_workers=4,\n",
    "    val_split=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c5983",
   "metadata": {},
   "source": [
    "# Complete unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77b5fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_unfrozen = copy.deepcopy(model_base)\n",
    "for param in model_unfrozen.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "trained_model_unfrozen, history_unfrozen = train_model_mlflow(\n",
    "    model=model_unfrozen,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=20,\n",
    "    lr=1e-3,\n",
    "    device=device,\n",
    "    experiment_name=\"resnet_cifar10\"\n",
    ")\n",
    "\n",
    "torch.save(trained_model_unfrozen.state_dict(), \"resnet_unfrozen.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f1b46",
   "metadata": {},
   "source": [
    "# Last layer unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c520dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_partial = copy.deepcopy(model_base)\n",
    "for name, param in model_partial.named_parameters():\n",
    "    if \"fc\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "trained_model_fc, history_fc = train_model_mlflow(\n",
    "    model=model_partial,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=20,\n",
    "    lr=1e-3,\n",
    "    device=device,\n",
    "    experiment_name=\"resnet_cifar10\"\n",
    ")\n",
    "\n",
    "torch.save(trained_model_fc.state_dict(), \"resnet_partial_freeze.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb8bfa0",
   "metadata": {},
   "source": [
    "# Layer4 and fc layer unfreeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766dea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Create partial-unfreeze model from model_base\n",
    "# -----------------------------------------------------\n",
    "model_partial = copy.deepcopy(model_base)\n",
    "\n",
    "# Freeze ALL layers first\n",
    "for param in model_partial.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze ONLY layer4 and fc\n",
    "for name, param in model_partial.named_parameters():\n",
    "    if \"layer4\" in name or \"fc\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Train using MLflow logging\n",
    "# -----------------------------------------------------\n",
    "trained_model_partial, history_partial = train_model_mlflow(\n",
    "    model=model_partial,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=20,\n",
    "    lr=1e-3,                 # use higher LR for unfrozen layers\n",
    "    device=device,\n",
    "    experiment_name=\"resnet_cifar10_partial_unfreeze\"\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Save final weights\n",
    "# -----------------------------------------------------\n",
    "torch.save(trained_model_partial.state_dict(), \"resnet_partial_unfreeze.pth\")\n",
    "\n",
    "print(\"✔ Partial-unfreeze training complete and model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7668f2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "# ---------------------------\n",
    "# Loss Curves\n",
    "# ---------------------------\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history_unfrozen['train_loss'], label='Train Loss (Unfrozen)')\n",
    "plt.plot(history_unfrozen['val_loss'], label='Val Loss (Unfrozen)')\n",
    "\n",
    "plt.plot(history_partial['train_loss'], label='Train Loss (FC Only)')\n",
    "plt.plot(history_partial['val_loss'], label='Val Loss (FC Only)')\n",
    "\n",
    "plt.plot(history_fc['train_loss'], label='Train Loss (Layer4 + FC)')\n",
    "plt.plot(history_fc['val_loss'], label='Val Loss (Layer4 + FC)')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curves')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Accuracy Curves\n",
    "# ---------------------------\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history_unfrozen['train_acc'], label='Train Acc (Unfrozen)')\n",
    "plt.plot(history_unfrozen['val_acc'], label='Val Acc (Unfrozen)')\n",
    "\n",
    "plt.plot(history_partial['train_acc'], label='Train Acc (FC Only)')\n",
    "plt.plot(history_partial['val_acc'], label='Val Acc (FC Only)')\n",
    "\n",
    "plt.plot(history_fc['train_acc'], label='Train Acc (Layer4 + FC)')\n",
    "plt.plot(history_fc['val_acc'], label='Val Acc (Layer4 + FC)')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Curves')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9f1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# Function to evaluate metrics\n",
    "# -----------------------------\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Overall accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f\"\\nTest Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Per-class metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, labels=range(10), average=None\n",
    "    )\n",
    "\n",
    "    # Get class names safely even if dataset is Subset\n",
    "    if isinstance(data_loader.dataset, torch.utils.data.Subset):\n",
    "        class_names = data_loader.dataset.dataset.classes\n",
    "    else:\n",
    "        class_names = data_loader.dataset.classes\n",
    "\n",
    "    print(\"\\nPer-class Metrics:\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        print(f\"{class_name}: Precision={precision[i]:.3f}, Recall={recall[i]:.3f}, F1={f1[i]:.3f}\")\n",
    "\n",
    "    # Macro and Weighted F1\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro'\n",
    "    )\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted'\n",
    "    )\n",
    "\n",
    "    print(f\"\\nMacro F1: {f1_macro:.3f}, Weighted F1: {f1_weighted:.3f}\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage for test set\n",
    "# -----------------------------\n",
    "print(\"Last FC layer unfreeze:\")\n",
    "acc_resnet_fc = evaluate_model(trained_model_fc, test_loader, device)\n",
    "\n",
    "print(\"Partial Layer4 + FC unfreeze:\")\n",
    "acc_resnet_partial = evaluate_model(trained_model_partial, test_loader, device)\n",
    "\n",
    "print(\"Fully unfrozen Fine-tuned ResNet:\")\n",
    "acc_resnet_unfrozen = evaluate_model(trained_model_unfrozen, test_loader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7686b064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "CIFAR10_MEAN = torch.tensor([0.4914, 0.4822, 0.4465]).view(3,1,1)\n",
    "CIFAR10_STD  = torch.tensor([0.2470, 0.2435, 0.2616]).view(3,1,1)\n",
    "\n",
    "def unnormalize(img):\n",
    "    return torch.clamp(img * CIFAR10_STD + CIFAR10_MEAN, 0, 1)\n",
    "\n",
    "def get_class_names(loader):\n",
    "    ds = loader.dataset\n",
    "\n",
    "    if hasattr(ds, \"classes\"):\n",
    "        return ds.classes\n",
    "\n",
    "    if hasattr(ds, \"dataset\") and hasattr(ds.dataset, \"classes\"):\n",
    "        return ds.dataset.classes\n",
    "\n",
    "    return [str(i) for i in range(10)]\n",
    "\n",
    "\n",
    "def show_misclassifications(model, data_loader, device, max_images=20):\n",
    "    class_names = get_class_names(data_loader)\n",
    "\n",
    "    model.eval()\n",
    "    wrong_images, wrong_preds, wrong_labels = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in data_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(imgs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            wrong_mask = preds != labels\n",
    "\n",
    "            idxs = wrong_mask.nonzero(as_tuple=False).flatten()\n",
    "            for idx in idxs:\n",
    "                wrong_images.append(imgs[idx].cpu())\n",
    "                wrong_preds.append(preds[idx].item())\n",
    "                wrong_labels.append(labels[idx].item())\n",
    "\n",
    "                if len(wrong_images) == max_images:\n",
    "                    break\n",
    "\n",
    "            if len(wrong_images) == max_images:\n",
    "                break\n",
    "\n",
    "    if len(wrong_images) == 0:\n",
    "        print(\"❗No misclassified images found.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i in range(len(wrong_images)):\n",
    "        plt.subplot(4, 5, i + 1)\n",
    "        img = unnormalize(wrong_images[i]).permute(1, 2, 0).numpy()\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Pred: {class_names[wrong_preds[i]]}\\nTrue: {class_names[wrong_labels[i]]}\",\n",
    "                  fontsize=9)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5cdfe9",
   "metadata": {},
   "source": [
    "# Misclassification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfb7385",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class_names = train_loader.dataset.dataset.classes  # handles Subset\n",
    "show_misclassifications(trained_model_unfrozen, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf3b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "# You need your unnormalize function for displaying images\n",
    "def unnormalize(tensor, mean=[0.4914, 0.4822, 0.4465], std=[0.2470, 0.2435, 0.2616]):\n",
    "    for t, m, s in zip(tensor, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return tensor.clamp(0, 1)\n",
    "\n",
    "def apply_gradcam_correct_vs_wrong(model, data_loader, class_names, device, max_images=20, alpha=0.3):\n",
    "    model.eval()\n",
    "    target_layer = model.layer4[-1]  # last conv layer\n",
    "    cam = GradCAM(model=model, target_layers=[target_layer])\n",
    "\n",
    "    correct_images, correct_cams, correct_preds = [], [], []\n",
    "    wrong_images, wrong_cams, wrong_preds, wrong_labels = [], [], [], []\n",
    "\n",
    "    for images, labels in data_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        for i in range(images.size(0)):\n",
    "            img = images[i].unsqueeze(0)  # 1xCxHxW\n",
    "            img.requires_grad = True\n",
    "\n",
    "            outputs = model(img)\n",
    "            pred = outputs.argmax(dim=1).item()\n",
    "            true_label = labels[i].item()\n",
    "\n",
    "            # Generate CAM for predicted class\n",
    "            targets = [ClassifierOutputTarget(pred)]\n",
    "            grayscale_cam = cam(input_tensor=img, targets=targets)[0, :, :]  # already numpy\n",
    "\n",
    "            # Resize to input size\n",
    "            cam_tensor = torch.tensor(grayscale_cam)[None, None, ...]  # 1x1xHxW\n",
    "            cam_resized = F.interpolate(cam_tensor, size=(224,224), mode='bilinear', align_corners=False)[0,0].numpy()\n",
    "\n",
    "            # Unnormalize for plotting\n",
    "            img_np = unnormalize(images[i].cpu()).permute(1,2,0).numpy()\n",
    "\n",
    "            # Separate correct and wrong predictions\n",
    "            if pred == true_label:\n",
    "                if len(correct_images) < max_images:\n",
    "                    correct_images.append(img_np)\n",
    "                    correct_cams.append(cam_resized)\n",
    "                    correct_preds.append(pred)\n",
    "            else:\n",
    "                if len(wrong_images) < max_images:\n",
    "                    wrong_images.append(img_np)\n",
    "                    wrong_cams.append(cam_resized)\n",
    "                    wrong_preds.append(pred)\n",
    "                    wrong_labels.append(true_label)\n",
    "\n",
    "            # Stop if both reached max_images\n",
    "            if len(correct_images) >= max_images and len(wrong_images) >= max_images:\n",
    "                break\n",
    "        if len(correct_images) >= max_images and len(wrong_images) >= max_images:\n",
    "            break\n",
    "\n",
    "    # Function to plot grid of images with CAM\n",
    "    def plot_grid(images_list, cams_list, preds_list, labels_list=None, title=\"\"):\n",
    "        cols = 5\n",
    "        rows = math.ceil(len(images_list)/cols)\n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols*4, rows*4))\n",
    "        axes = axes.flatten()\n",
    "        for idx in range(len(images_list)):\n",
    "            axes[idx].imshow(images_list[idx])\n",
    "            axes[idx].imshow(cams_list[idx], cmap='jet', alpha=alpha)\n",
    "            if labels_list is not None:\n",
    "                axes[idx].set_title(f\"Pred: {class_names[preds_list[idx]]}\\nTrue: {class_names[labels_list[idx]]}\", fontsize=9)\n",
    "            else:\n",
    "                axes[idx].set_title(f\"Pred: {class_names[preds_list[idx]]}\", fontsize=9)\n",
    "            axes[idx].axis('off')\n",
    "        # Turn off extra axes\n",
    "        for idx in range(len(images_list), len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        plt.suptitle(title, fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "     # Plot correct predictions\n",
    "    if correct_images:\n",
    "        plot_grid(correct_images, correct_cams, correct_preds, title=\"Grad-CAM: Correctly Classified Images\")\n",
    "\n",
    "    # Plot misclassified predictions with true labels\n",
    "    if wrong_images:\n",
    "        plot_grid(wrong_images, wrong_cams, wrong_preds, labels_list=wrong_labels,\n",
    "                  title=\"Grad-CAM: Misclassified Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6d100d",
   "metadata": {},
   "source": [
    "# Grad-Cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bee5463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = test_loader.dataset.classes\n",
    "apply_gradcam_correct_vs_wrong(trained_model_unfrozen, test_loader, class_names, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7769275b",
   "metadata": {},
   "source": [
    "# Adversarial Sanity Check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3241652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Function to compute metrics on original and FGSM adversarial examples\n",
    "# -----------------------------\n",
    "def adversarial_metrics(model, data_loader, device, epsilon=0.03):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred_original = []\n",
    "    y_pred_adv = []\n",
    "\n",
    "    for images, labels in data_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        images.requires_grad = True\n",
    "\n",
    "        # --- Forward on original ---\n",
    "        outputs = model(images)\n",
    "        preds_orig = outputs.argmax(dim=1)\n",
    "        \n",
    "        # --- FGSM attack ---\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        perturbation = epsilon * images.grad.sign()\n",
    "        adv_images = torch.clamp(images + perturbation, 0, 1)\n",
    "\n",
    "        # --- Forward on adversarial ---\n",
    "        with torch.no_grad():\n",
    "            adv_outputs = model(adv_images)\n",
    "            preds_adv = adv_outputs.argmax(dim=1)\n",
    "\n",
    "        # --- Collect labels and predictions ---\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred_original.extend(preds_orig.cpu().numpy())\n",
    "        y_pred_adv.extend(preds_adv.cpu().numpy())\n",
    "\n",
    "        # Free memory\n",
    "        del images, labels, outputs, loss, adv_images, adv_outputs, perturbation\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Metrics calculation\n",
    "    def calc_metrics(y_true, y_pred):\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "        return acc, precision, recall, f1\n",
    "\n",
    "    return calc_metrics(y_true, y_pred_original), calc_metrics(y_true, y_pred_adv)\n",
    "\n",
    "# Compute for all models\n",
    "models = {\n",
    "    \"Fully Unfrozen\": trained_model_unfrozen,\n",
    "    \"Partial Layer4+FC\": trained_model_partial,\n",
    "    \"Last FC Only\": trained_model_fc\n",
    "}\n",
    "\n",
    "metrics_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    orig_metrics, adv_metrics = adversarial_metrics(model, test_loader, device, epsilon=0.03)\n",
    "    metrics_results[name] = {\"original\": orig_metrics, \"adversarial\": adv_metrics}\n",
    "    print(f\"\\n{name}:\\nOriginal: {orig_metrics}\\nAdversarial: {adv_metrics}\")\n",
    "\n",
    "# Plot comparison graphs\n",
    "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "for i, metric in enumerate(metrics_names):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    orig_vals = [metrics_results[m]['original'][i] for m in models]\n",
    "    adv_vals = [metrics_results[m]['adversarial'][i] for m in models]\n",
    "\n",
    "    plt.bar(x - width/2, orig_vals, width, label='Original')\n",
    "    plt.bar(x + width/2, adv_vals, width, label='Adversarial')\n",
    "    plt.xticks(x, models.keys(), rotation=15)\n",
    "    plt.ylabel(metric)\n",
    "    plt.ylim(0,1)\n",
    "    plt.title(f\"{metric} Comparison\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bf3f31",
   "metadata": {},
   "source": [
    "# Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec98be0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# Function to compute ECE\n",
    "# -----------------------------\n",
    "def compute_ece(model, data_loader, device, n_bins=15):\n",
    "    \"\"\"\n",
    "    Expected Calibration Error (ECE) for multiclass classification\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    confidences = []\n",
    "    predictions = []\n",
    "    labels_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            confs, preds = torch.max(probs, dim=1)\n",
    "            \n",
    "            confidences.extend(confs.cpu().numpy())\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            labels_list.extend(labels.numpy())\n",
    "\n",
    "    confidences = np.array(confidences)\n",
    "    predictions = np.array(predictions)\n",
    "    labels_list = np.array(labels_list)\n",
    "    \n",
    "    # ECE calculation\n",
    "    bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        bin_lower = bin_edges[i]\n",
    "        bin_upper = bin_edges[i+1]\n",
    "        mask = (confidences > bin_lower) & (confidences <= bin_upper)\n",
    "        if np.any(mask):\n",
    "            accuracy_in_bin = (predictions[mask] == labels_list[mask]).mean()\n",
    "            avg_confidence_in_bin = confidences[mask].mean()\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * mask.mean()\n",
    "    \n",
    "    return ece, confidences, predictions, labels_list\n",
    "\n",
    "# -----------------------------\n",
    "# Plot reliability diagram\n",
    "# -----------------------------\n",
    "def plot_reliability_diagram(confidences, predictions, labels, n_bins=15, title=\"Reliability Diagram\"):\n",
    "    bin_edges = np.linspace(0,1,n_bins+1)\n",
    "    acc_bins = np.zeros(n_bins)\n",
    "    conf_bins = np.zeros(n_bins)\n",
    "    counts = np.zeros(n_bins)\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        mask = (confidences > bin_edges[i]) & (confidences <= bin_edges[i+1])\n",
    "        if np.any(mask):\n",
    "            acc_bins[i] = (predictions[mask] == labels[mask]).mean()\n",
    "            conf_bins[i] = confidences[mask].mean()\n",
    "            counts[i] = mask.mean()\n",
    "\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot([0,1],[0,1], linestyle='--', color='gray')\n",
    "    plt.plot(conf_bins, acc_bins, marker='o', label='Model')\n",
    "    plt.fill_between(conf_bins, acc_bins, conf_bins, alpha=0.2, color='blue')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# Compute & plot for all models\n",
    "# -----------------------------\n",
    "models = {\n",
    "    \"Fully Unfrozen\": trained_model_unfrozen,\n",
    "    \"Partial Layer4+FC\": trained_model_partial,\n",
    "    \"Last FC Only\": trained_model_fc\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    ece, confs, preds, labels = compute_ece(model, test_loader, device, n_bins=15)\n",
    "    print(f\"{name} ECE: {ece:.4f}\")\n",
    "    plot_reliability_diagram(confs, preds, labels, n_bins=15, title=f\"{name} Reliability Diagram\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f916ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "# -----------------------------\n",
    "# Expected Calibration Error\n",
    "# -----------------------------\n",
    "def compute_ece(probs, labels, n_bins=15):\n",
    "    \"\"\"\n",
    "    Computes Expected Calibration Error (ECE)\n",
    "    probs: [N, C] predicted probabilities\n",
    "    labels: [N] true labels\n",
    "    \"\"\"\n",
    "    confidences, predictions = torch.max(probs, 1)\n",
    "    labels = labels.cpu().numpy()\n",
    "    confidences = confidences.cpu().numpy()\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    \n",
    "    ece = 0.0\n",
    "    bin_edges = np.linspace(0, 1, n_bins+1)\n",
    "    for i in range(n_bins):\n",
    "        mask = (confidences > bin_edges[i]) & (confidences <= bin_edges[i+1])\n",
    "        if np.sum(mask) > 0:\n",
    "            acc = np.mean(predictions[mask] == labels[mask])\n",
    "            conf = np.mean(confidences[mask])\n",
    "            ece += np.sum(mask) / len(labels) * np.abs(acc - conf)\n",
    "    return ece\n",
    "\n",
    "# -----------------------------\n",
    "# Temperature scaling wrapper\n",
    "# -----------------------------\n",
    "class ModelWithTemperature(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.temperature = nn.Parameter(torch.ones(1))  # initialized as 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        return self.temperature_scale(logits)\n",
    "\n",
    "    def temperature_scale(self, logits):\n",
    "        return logits / self.temperature\n",
    "\n",
    "    def set_temperature(self, valid_loader, device):\n",
    "        self.to(device)\n",
    "        nll_criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.LBFGS([self.temperature], lr=0.01, max_iter=50)\n",
    "\n",
    "        # Collect logits and labels without gradient for model\n",
    "        logits_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                logits = self.model(images)\n",
    "                logits_list.append(logits)\n",
    "                labels_list.append(labels)\n",
    "\n",
    "        logits_all = torch.cat(logits_list)\n",
    "        labels_all = torch.cat(labels_list)\n",
    "\n",
    "        def eval():\n",
    "            optimizer.zero_grad()\n",
    "            loss = nll_criterion(self.temperature_scale(logits_all), labels_all)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(eval)\n",
    "        print(f\"Optimal temperature: {self.temperature.item():.3f}\")\n",
    "        return self\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation function\n",
    "# -----------------------------\n",
    "def evaluate_model_with_calibration(model, data_loader, device, n_bins=15):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            all_probs.append(probs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    all_probs = torch.cat(all_probs)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    ece = compute_ece(all_probs, all_labels, n_bins=n_bins)\n",
    "    print(f\"ECE: {ece:.4f}\")\n",
    "\n",
    "    # Reliability diagram\n",
    "    plt.figure(figsize=(6,6))\n",
    "    for i in range(all_probs.shape[1]):  # per-class calibration curve\n",
    "        frac_pos, mean_conf = calibration_curve(\n",
    "            (all_labels.cpu().numpy() == i),\n",
    "            all_probs[:, i].cpu().numpy(),\n",
    "            n_bins=n_bins\n",
    "        )\n",
    "        plt.plot(mean_conf, frac_pos, marker='o', label=f\"Class {i}\")\n",
    "    plt.plot([0,1],[0,1],'k--')\n",
    "    plt.xlabel(\"Mean predicted probability\")\n",
    "    plt.ylabel(\"Fraction of positives\")\n",
    "    plt.title(\"Reliability Diagram\")\n",
    "    plt.legend(bbox_to_anchor=(1.05,1), loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    return ece\n",
    "\n",
    "# -----------------------------\n",
    "# Apply temperature scaling and evaluate all models\n",
    "# -----------------------------\n",
    "models_dict = {\n",
    "    \"FC Only\": trained_model_fc,\n",
    "    \"Layer4 + FC\": trained_model_partial,\n",
    "    \"Fully Unfrozen\": trained_model_unfrozen\n",
    "}\n",
    "\n",
    "ece_results = {}\n",
    "\n",
    "for name, model in models_dict.items():\n",
    "    print(f\"\\nProcessing model: {name}\")\n",
    "    temp_model = ModelWithTemperature(model)\n",
    "    temp_model.set_temperature(val_loader, device)\n",
    "    ece = evaluate_model_with_calibration(temp_model, test_loader, device)\n",
    "    ece_results[name] = ece\n",
    "\n",
    "# -----------------------------\n",
    "# Plot comparison of ECEs\n",
    "# -----------------------------\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.bar(ece_results.keys(), ece_results.values(), color=['skyblue','orange','green'])\n",
    "plt.ylabel(\"ECE\")\n",
    "plt.title(\"ECE Comparison Across Models After Temperature Scaling\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e45caf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
